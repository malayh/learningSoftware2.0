{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(r'data\\names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(word : str) -> list[tuple[str,str]]:\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    pairs = [(i,j,k) for i,j,k in zip(chars,chars[1:],chars[2:]) ]\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# make bigram stat\n",
    "trigram_counts = {}\n",
    "for word in words:\n",
    "    triplets = get_trigrams(word)\n",
    "    for triplet in triplets:\n",
    "        trigram_counts[triplet] = trigram_counts.get(triplet,0) + 1\n",
    "\n",
    "\n",
    "# make char -> index lookup table\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "chars = ['.'] + chars\n",
    "\n",
    "char2idx = {c:i for i,c in enumerate(chars)}\n",
    "idx2char = {i:c for c,i in char2idx.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Neural net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total bigrams: 196113\n",
      "torch.Size([196113, 54])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    triplets = get_trigrams(w)\n",
    "    for triplet in triplets:\n",
    "        x1,x2,y = char2idx[triplet[0]],char2idx[triplet[1]],char2idx[triplet[2]]\n",
    "        xs.append([x1,x2])\n",
    "        ys.append(y)\n",
    "\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "x_enc = F.one_hot(xs, num_classes=27).float().flatten(1,2)\n",
    "\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "total_trigrams = len(ys)\n",
    "print(f'total bigrams: {total_trigrams}')\n",
    "print(x_enc.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_gen = torch.Generator().manual_seed(1234)\n",
    "weights = torch.randn((54,27), generator=rand_gen,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.3784940242767334\n",
      "tensor(2.3785, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(100):\n",
    "    # forward pass\n",
    "\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    logits = x_enc @ weights\n",
    "    \n",
    "    # This is cross entropy loss\n",
    "    # count = logits.exp()\n",
    "    # probs = count / count.sum(dim=1,keepdim=True)\n",
    "    # loss = -probs[torch.arange(total_trigrams),ys].log().mean()\n",
    "\n",
    "    loss = loss_fn(logits,ys) + 0.01 * (weights**2).mean()\n",
    "\n",
    "    print(f'epoch: {epoch}, loss: {loss.item()}')\n",
    "\n",
    "    # backward pass\n",
    "    weights.grad = None\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # update weights\n",
    "    weights.data -= 10 * weights.grad\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word names with l \n",
      " -----------------\n",
      "laim\n",
      "la\n",
      "le\n",
      "lyna\n",
      "liue\n",
      "lonn\n",
      "la\n",
      "luy\n",
      "lyaila\n",
      "lin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inferencing\n",
    "\n",
    "rand_gen = torch.Generator().manual_seed(12141234)\n",
    "\n",
    "def make_word(start_with : str) -> str:\n",
    "    start = [0,char2idx[start_with]]\n",
    "    out = [start_with]\n",
    "\n",
    "    while True:\n",
    "        x_enc = F.one_hot(torch.tensor(start), num_classes=27).float().flatten()\n",
    "        logits = x_enc @ weights\n",
    "        count = logits.exp()\n",
    "        probs = count / count.sum()\n",
    "\n",
    "        next_char = torch.multinomial(probs,1, generator=rand_gen, replacement=True).item()\n",
    "\n",
    "        if next_char == 0:\n",
    "            break\n",
    "\n",
    "        out.append(idx2char[next_char])\n",
    "        start = [start[1],next_char]\n",
    "    \n",
    "    print(''.join(out))\n",
    "\n",
    "\n",
    "\n",
    "_char = 'l'\n",
    "print(f'Word names with {_char} \\n -----------------')\n",
    "for i in range(10):\n",
    "    make_word(_char)\n",
    "print()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7afcb4a04d08665c2529ae0dc1d5ec01f94baaa4a57a18e0d9f75f04722700b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
