{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(r'data\\names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(word : str) -> list[tuple[str,str]]:\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    pairs = [(i,j,k) for i,j,k in zip(chars,chars[1:],chars[2:]) ]\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# make bigram stat\n",
    "trigram_counts = {}\n",
    "for word in words:\n",
    "    triplets = get_trigrams(word)\n",
    "    for triplet in triplets:\n",
    "        trigram_counts[triplet] = trigram_counts.get(triplet,0) + 1\n",
    "\n",
    "\n",
    "# make char -> index lookup table\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "chars = ['.'] + chars\n",
    "\n",
    "char2idx = {c:i for i,c in enumerate(chars)}\n",
    "idx2char = {i:c for c,i in char2idx.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Neural net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total bigrams: 196113\n",
      "torch.Size([196113, 54])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    triplets = get_trigrams(w)\n",
    "    for triplet in triplets:\n",
    "        x1,x2,y = char2idx[triplet[0]],char2idx[triplet[1]],char2idx[triplet[2]]\n",
    "        xs.append([x1,x2])\n",
    "        ys.append(y)\n",
    "\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "x_enc = F.one_hot(xs, num_classes=27).float().flatten(1,2)\n",
    "\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "total_trigrams = len(ys)\n",
    "print(f'total bigrams: {total_trigrams}')\n",
    "print(x_enc.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_gen = torch.Generator().manual_seed(1234)\n",
    "weights = torch.randn((54,27), generator=rand_gen,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.2570247650146484\n",
      "epoch: 1, loss: 2.25701904296875\n",
      "epoch: 2, loss: 2.257012367248535\n",
      "epoch: 3, loss: 2.2570064067840576\n",
      "epoch: 4, loss: 2.25700044631958\n",
      "epoch: 5, loss: 2.2569942474365234\n",
      "epoch: 6, loss: 2.2569878101348877\n",
      "epoch: 7, loss: 2.25698184967041\n",
      "epoch: 8, loss: 2.2569758892059326\n",
      "epoch: 9, loss: 2.256969928741455\n",
      "epoch: 10, loss: 2.2569634914398193\n",
      "epoch: 11, loss: 2.256957530975342\n",
      "epoch: 12, loss: 2.2569515705108643\n",
      "epoch: 13, loss: 2.2569456100463867\n",
      "epoch: 14, loss: 2.25693941116333\n",
      "epoch: 15, loss: 2.2569339275360107\n",
      "epoch: 16, loss: 2.256927728652954\n",
      "epoch: 17, loss: 2.2569215297698975\n",
      "epoch: 18, loss: 2.256916046142578\n",
      "epoch: 19, loss: 2.2569096088409424\n",
      "epoch: 20, loss: 2.256903648376465\n",
      "epoch: 21, loss: 2.2568976879119873\n",
      "epoch: 22, loss: 2.2568917274475098\n",
      "epoch: 23, loss: 2.2568860054016113\n",
      "epoch: 24, loss: 2.2568798065185547\n",
      "epoch: 25, loss: 2.2568740844726562\n",
      "epoch: 26, loss: 2.256868362426758\n",
      "epoch: 27, loss: 2.2568624019622803\n",
      "epoch: 28, loss: 2.2568559646606445\n",
      "epoch: 29, loss: 2.256850481033325\n",
      "epoch: 30, loss: 2.2568447589874268\n",
      "epoch: 31, loss: 2.256838798522949\n",
      "epoch: 32, loss: 2.256833076477051\n",
      "epoch: 33, loss: 2.2568271160125732\n",
      "epoch: 34, loss: 2.256821393966675\n",
      "epoch: 35, loss: 2.2568154335021973\n",
      "epoch: 36, loss: 2.256809711456299\n",
      "epoch: 37, loss: 2.2568039894104004\n",
      "epoch: 38, loss: 2.256798267364502\n",
      "epoch: 39, loss: 2.2567925453186035\n",
      "epoch: 40, loss: 2.256786584854126\n",
      "epoch: 41, loss: 2.2567811012268066\n",
      "epoch: 42, loss: 2.256775140762329\n",
      "epoch: 43, loss: 2.2567696571350098\n",
      "epoch: 44, loss: 2.2567639350891113\n",
      "epoch: 45, loss: 2.256758213043213\n",
      "epoch: 46, loss: 2.2567522525787354\n",
      "epoch: 47, loss: 2.256747007369995\n",
      "epoch: 48, loss: 2.2567412853240967\n",
      "epoch: 49, loss: 2.256735324859619\n",
      "epoch: 50, loss: 2.2567298412323\n",
      "epoch: 51, loss: 2.2567241191864014\n",
      "epoch: 52, loss: 2.256718635559082\n",
      "epoch: 53, loss: 2.2567129135131836\n",
      "epoch: 54, loss: 2.2567074298858643\n",
      "epoch: 55, loss: 2.256701946258545\n",
      "epoch: 56, loss: 2.2566962242126465\n",
      "epoch: 57, loss: 2.256690740585327\n",
      "epoch: 58, loss: 2.256685256958008\n",
      "epoch: 59, loss: 2.2566792964935303\n",
      "epoch: 60, loss: 2.25667405128479\n",
      "epoch: 61, loss: 2.2566685676574707\n",
      "epoch: 62, loss: 2.2566630840301514\n",
      "epoch: 63, loss: 2.256657600402832\n",
      "epoch: 64, loss: 2.2566518783569336\n",
      "epoch: 65, loss: 2.256646156311035\n",
      "epoch: 66, loss: 2.256640672683716\n",
      "epoch: 67, loss: 2.2566351890563965\n",
      "epoch: 68, loss: 2.256629705429077\n",
      "epoch: 69, loss: 2.256624698638916\n",
      "epoch: 70, loss: 2.2566187381744385\n",
      "epoch: 71, loss: 2.2566137313842773\n",
      "epoch: 72, loss: 2.256608247756958\n",
      "epoch: 73, loss: 2.2566030025482178\n",
      "epoch: 74, loss: 2.2565972805023193\n",
      "epoch: 75, loss: 2.256592035293579\n",
      "epoch: 76, loss: 2.256586790084839\n",
      "epoch: 77, loss: 2.2565815448760986\n",
      "epoch: 78, loss: 2.2565760612487793\n",
      "epoch: 79, loss: 2.25657057762146\n",
      "epoch: 80, loss: 2.2565648555755615\n",
      "epoch: 81, loss: 2.2565598487854004\n",
      "epoch: 82, loss: 2.2565548419952393\n",
      "epoch: 83, loss: 2.25654935836792\n",
      "epoch: 84, loss: 2.2565438747406006\n",
      "epoch: 85, loss: 2.2565386295318604\n",
      "epoch: 86, loss: 2.256533145904541\n",
      "epoch: 87, loss: 2.25652813911438\n",
      "epoch: 88, loss: 2.2565228939056396\n",
      "epoch: 89, loss: 2.2565174102783203\n",
      "epoch: 90, loss: 2.256512403488159\n",
      "epoch: 91, loss: 2.25650691986084\n",
      "epoch: 92, loss: 2.2565019130706787\n",
      "epoch: 93, loss: 2.2564966678619385\n",
      "epoch: 94, loss: 2.2564914226531982\n",
      "epoch: 95, loss: 2.256485939025879\n",
      "epoch: 96, loss: 2.2564809322357178\n",
      "epoch: 97, loss: 2.2564756870269775\n",
      "epoch: 98, loss: 2.2564706802368164\n",
      "epoch: 99, loss: 2.256465435028076\n",
      "epoch: 100, loss: 2.256460428237915\n",
      "epoch: 101, loss: 2.256455183029175\n",
      "epoch: 102, loss: 2.2564499378204346\n",
      "epoch: 103, loss: 2.2564446926116943\n",
      "epoch: 104, loss: 2.256439447402954\n",
      "epoch: 105, loss: 2.256434440612793\n",
      "epoch: 106, loss: 2.256429433822632\n",
      "epoch: 107, loss: 2.2564241886138916\n",
      "epoch: 108, loss: 2.2564189434051514\n",
      "epoch: 109, loss: 2.2564141750335693\n",
      "epoch: 110, loss: 2.256408929824829\n",
      "epoch: 111, loss: 2.256404161453247\n",
      "epoch: 112, loss: 2.256399154663086\n",
      "epoch: 113, loss: 2.2563939094543457\n",
      "epoch: 114, loss: 2.2563889026641846\n",
      "epoch: 115, loss: 2.2563838958740234\n",
      "epoch: 116, loss: 2.2563788890838623\n",
      "epoch: 117, loss: 2.256373882293701\n",
      "epoch: 118, loss: 2.256368637084961\n",
      "epoch: 119, loss: 2.256363868713379\n",
      "epoch: 120, loss: 2.2563588619232178\n",
      "epoch: 121, loss: 2.2563540935516357\n",
      "epoch: 122, loss: 2.2563488483428955\n",
      "epoch: 123, loss: 2.2563440799713135\n",
      "epoch: 124, loss: 2.2563390731811523\n",
      "epoch: 125, loss: 2.2563343048095703\n",
      "epoch: 126, loss: 2.256329298019409\n",
      "epoch: 127, loss: 2.256324052810669\n",
      "epoch: 128, loss: 2.256319046020508\n",
      "epoch: 129, loss: 2.256314277648926\n",
      "epoch: 130, loss: 2.2563095092773438\n",
      "epoch: 131, loss: 2.2563047409057617\n",
      "epoch: 132, loss: 2.2562997341156006\n",
      "epoch: 133, loss: 2.2562949657440186\n",
      "epoch: 134, loss: 2.2562899589538574\n",
      "epoch: 135, loss: 2.2562851905822754\n",
      "epoch: 136, loss: 2.2562801837921143\n",
      "epoch: 137, loss: 2.2562754154205322\n",
      "epoch: 138, loss: 2.25627064704895\n",
      "epoch: 139, loss: 2.256265878677368\n",
      "epoch: 140, loss: 2.256260871887207\n",
      "epoch: 141, loss: 2.256256341934204\n",
      "epoch: 142, loss: 2.256251573562622\n",
      "epoch: 143, loss: 2.256246328353882\n",
      "epoch: 144, loss: 2.256241798400879\n",
      "epoch: 145, loss: 2.256237030029297\n",
      "epoch: 146, loss: 2.2562320232391357\n",
      "epoch: 147, loss: 2.2562272548675537\n",
      "epoch: 148, loss: 2.256222724914551\n",
      "epoch: 149, loss: 2.2562177181243896\n",
      "epoch: 150, loss: 2.2562131881713867\n",
      "epoch: 151, loss: 2.256208658218384\n",
      "epoch: 152, loss: 2.2562038898468018\n",
      "epoch: 153, loss: 2.2561988830566406\n",
      "epoch: 154, loss: 2.2561943531036377\n",
      "epoch: 155, loss: 2.2561898231506348\n",
      "epoch: 156, loss: 2.2561850547790527\n",
      "epoch: 157, loss: 2.2561798095703125\n",
      "epoch: 158, loss: 2.2561757564544678\n",
      "epoch: 159, loss: 2.256171226501465\n",
      "epoch: 160, loss: 2.256166458129883\n",
      "epoch: 161, loss: 2.256161689758301\n",
      "epoch: 162, loss: 2.256157159805298\n",
      "epoch: 163, loss: 2.2561521530151367\n",
      "epoch: 164, loss: 2.256147623062134\n",
      "epoch: 165, loss: 2.25614333152771\n",
      "epoch: 166, loss: 2.256138563156128\n",
      "epoch: 167, loss: 2.256134033203125\n",
      "epoch: 168, loss: 2.256129503250122\n",
      "epoch: 169, loss: 2.256124496459961\n",
      "epoch: 170, loss: 2.256119966506958\n",
      "epoch: 171, loss: 2.256115674972534\n",
      "epoch: 172, loss: 2.256110906600952\n",
      "epoch: 173, loss: 2.2561066150665283\n",
      "epoch: 174, loss: 2.2561018466949463\n",
      "epoch: 175, loss: 2.2560970783233643\n",
      "epoch: 176, loss: 2.2560927867889404\n",
      "epoch: 177, loss: 2.2560882568359375\n",
      "epoch: 178, loss: 2.2560839653015137\n",
      "epoch: 179, loss: 2.2560794353485107\n",
      "epoch: 180, loss: 2.256074905395508\n",
      "epoch: 181, loss: 2.256070613861084\n",
      "epoch: 182, loss: 2.256065845489502\n",
      "epoch: 183, loss: 2.256061553955078\n",
      "epoch: 184, loss: 2.256057024002075\n",
      "epoch: 185, loss: 2.2560524940490723\n",
      "epoch: 186, loss: 2.2560479640960693\n",
      "epoch: 187, loss: 2.2560434341430664\n",
      "epoch: 188, loss: 2.2560391426086426\n",
      "epoch: 189, loss: 2.2560346126556396\n",
      "epoch: 190, loss: 2.256030321121216\n",
      "epoch: 191, loss: 2.256025791168213\n",
      "epoch: 192, loss: 2.25602126121521\n",
      "epoch: 193, loss: 2.256016969680786\n",
      "epoch: 194, loss: 2.2560126781463623\n",
      "epoch: 195, loss: 2.2560083866119385\n",
      "epoch: 196, loss: 2.2560036182403564\n",
      "epoch: 197, loss: 2.255999803543091\n",
      "epoch: 198, loss: 2.255995273590088\n",
      "epoch: 199, loss: 2.255990743637085\n",
      "epoch: 200, loss: 2.255986452102661\n",
      "epoch: 201, loss: 2.255981922149658\n",
      "epoch: 202, loss: 2.2559776306152344\n",
      "epoch: 203, loss: 2.2559733390808105\n",
      "epoch: 204, loss: 2.255969285964966\n",
      "epoch: 205, loss: 2.255964756011963\n",
      "epoch: 206, loss: 2.255960464477539\n",
      "epoch: 207, loss: 2.2559561729431152\n",
      "epoch: 208, loss: 2.2559518814086914\n",
      "epoch: 209, loss: 2.2559475898742676\n",
      "epoch: 210, loss: 2.2559432983398438\n",
      "epoch: 211, loss: 2.255938768386841\n",
      "epoch: 212, loss: 2.255934953689575\n",
      "epoch: 213, loss: 2.2559304237365723\n",
      "epoch: 214, loss: 2.2559261322021484\n",
      "epoch: 215, loss: 2.2559220790863037\n",
      "epoch: 216, loss: 2.25591778755188\n",
      "epoch: 217, loss: 2.255913496017456\n",
      "epoch: 218, loss: 2.2559092044830322\n",
      "epoch: 219, loss: 2.2559051513671875\n",
      "epoch: 220, loss: 2.2559008598327637\n",
      "epoch: 221, loss: 2.25589656829834\n",
      "epoch: 222, loss: 2.255892276763916\n",
      "epoch: 223, loss: 2.2558882236480713\n",
      "epoch: 224, loss: 2.2558839321136475\n",
      "epoch: 225, loss: 2.2558798789978027\n",
      "epoch: 226, loss: 2.255875825881958\n",
      "epoch: 227, loss: 2.255871534347534\n",
      "epoch: 228, loss: 2.2558674812316895\n",
      "epoch: 229, loss: 2.2558631896972656\n",
      "epoch: 230, loss: 2.255859136581421\n",
      "epoch: 231, loss: 2.255854845046997\n",
      "epoch: 232, loss: 2.2558507919311523\n",
      "epoch: 233, loss: 2.2558467388153076\n",
      "epoch: 234, loss: 2.255842685699463\n",
      "epoch: 235, loss: 2.255838632583618\n",
      "epoch: 236, loss: 2.2558343410491943\n",
      "epoch: 237, loss: 2.2558300495147705\n",
      "epoch: 238, loss: 2.255826234817505\n",
      "epoch: 239, loss: 2.25582218170166\n",
      "epoch: 240, loss: 2.2558178901672363\n",
      "epoch: 241, loss: 2.2558140754699707\n",
      "epoch: 242, loss: 2.255810022354126\n",
      "epoch: 243, loss: 2.255805730819702\n",
      "epoch: 244, loss: 2.2558016777038574\n",
      "epoch: 245, loss: 2.2557976245880127\n",
      "epoch: 246, loss: 2.255793809890747\n",
      "epoch: 247, loss: 2.2557897567749023\n",
      "epoch: 248, loss: 2.2557857036590576\n",
      "epoch: 249, loss: 2.255781888961792\n",
      "epoch: 250, loss: 2.2557778358459473\n",
      "epoch: 251, loss: 2.2557735443115234\n",
      "epoch: 252, loss: 2.255769729614258\n",
      "epoch: 253, loss: 2.255765676498413\n",
      "epoch: 254, loss: 2.2557618618011475\n",
      "epoch: 255, loss: 2.2557575702667236\n",
      "epoch: 256, loss: 2.255753755569458\n",
      "epoch: 257, loss: 2.2557499408721924\n",
      "epoch: 258, loss: 2.2557458877563477\n",
      "epoch: 259, loss: 2.255742073059082\n",
      "epoch: 260, loss: 2.2557380199432373\n",
      "epoch: 261, loss: 2.2557342052459717\n",
      "epoch: 262, loss: 2.255730152130127\n",
      "epoch: 263, loss: 2.2557263374328613\n",
      "epoch: 264, loss: 2.2557222843170166\n",
      "epoch: 265, loss: 2.255718469619751\n",
      "epoch: 266, loss: 2.2557144165039062\n",
      "epoch: 267, loss: 2.2557103633880615\n",
      "epoch: 268, loss: 2.255706787109375\n",
      "epoch: 269, loss: 2.2557029724121094\n",
      "epoch: 270, loss: 2.2556986808776855\n",
      "epoch: 271, loss: 2.255695104598999\n",
      "epoch: 272, loss: 2.2556915283203125\n",
      "epoch: 273, loss: 2.2556872367858887\n",
      "epoch: 274, loss: 2.255683422088623\n",
      "epoch: 275, loss: 2.2556793689727783\n",
      "epoch: 276, loss: 2.255676031112671\n",
      "epoch: 277, loss: 2.255671977996826\n",
      "epoch: 278, loss: 2.2556681632995605\n",
      "epoch: 279, loss: 2.255664348602295\n",
      "epoch: 280, loss: 2.2556605339050293\n",
      "epoch: 281, loss: 2.2556567192077637\n",
      "epoch: 282, loss: 2.255652666091919\n",
      "epoch: 283, loss: 2.2556490898132324\n",
      "epoch: 284, loss: 2.2556450366973877\n",
      "epoch: 285, loss: 2.255641460418701\n",
      "epoch: 286, loss: 2.2556376457214355\n",
      "epoch: 287, loss: 2.25563383102417\n",
      "epoch: 288, loss: 2.2556300163269043\n",
      "epoch: 289, loss: 2.2556262016296387\n",
      "epoch: 290, loss: 2.255622625350952\n",
      "epoch: 291, loss: 2.2556188106536865\n",
      "epoch: 292, loss: 2.255614995956421\n",
      "epoch: 293, loss: 2.2556111812591553\n",
      "epoch: 294, loss: 2.2556076049804688\n",
      "epoch: 295, loss: 2.255603551864624\n",
      "epoch: 296, loss: 2.2556002140045166\n",
      "epoch: 297, loss: 2.255596399307251\n",
      "epoch: 298, loss: 2.2555928230285645\n",
      "epoch: 299, loss: 2.255589008331299\n",
      "epoch: 300, loss: 2.255585193634033\n",
      "epoch: 301, loss: 2.255581855773926\n",
      "epoch: 302, loss: 2.255577802658081\n",
      "epoch: 303, loss: 2.2555742263793945\n",
      "epoch: 304, loss: 2.255570411682129\n",
      "epoch: 305, loss: 2.2555668354034424\n",
      "epoch: 306, loss: 2.255563497543335\n",
      "epoch: 307, loss: 2.2555596828460693\n",
      "epoch: 308, loss: 2.2555556297302246\n",
      "epoch: 309, loss: 2.255552291870117\n",
      "epoch: 310, loss: 2.2555487155914307\n",
      "epoch: 311, loss: 2.255545139312744\n",
      "epoch: 312, loss: 2.2555415630340576\n",
      "epoch: 313, loss: 2.255537748336792\n",
      "epoch: 314, loss: 2.2555339336395264\n",
      "epoch: 315, loss: 2.25553035736084\n",
      "epoch: 316, loss: 2.255526542663574\n",
      "epoch: 317, loss: 2.255523204803467\n",
      "epoch: 318, loss: 2.2555198669433594\n",
      "epoch: 319, loss: 2.255516290664673\n",
      "epoch: 320, loss: 2.2555124759674072\n",
      "epoch: 321, loss: 2.2555088996887207\n",
      "epoch: 322, loss: 2.255505323410034\n",
      "epoch: 323, loss: 2.2555017471313477\n",
      "epoch: 324, loss: 2.2554984092712402\n",
      "epoch: 325, loss: 2.2554945945739746\n",
      "epoch: 326, loss: 2.255491256713867\n",
      "epoch: 327, loss: 2.2554876804351807\n",
      "epoch: 328, loss: 2.255483865737915\n",
      "epoch: 329, loss: 2.2554805278778076\n",
      "epoch: 330, loss: 2.255476713180542\n",
      "epoch: 331, loss: 2.2554736137390137\n",
      "epoch: 332, loss: 2.2554702758789062\n",
      "epoch: 333, loss: 2.2554664611816406\n",
      "epoch: 334, loss: 2.255463123321533\n",
      "epoch: 335, loss: 2.2554595470428467\n",
      "epoch: 336, loss: 2.25545597076416\n",
      "epoch: 337, loss: 2.255452871322632\n",
      "epoch: 338, loss: 2.255449056625366\n",
      "epoch: 339, loss: 2.2554454803466797\n",
      "epoch: 340, loss: 2.2554421424865723\n",
      "epoch: 341, loss: 2.2554385662078857\n",
      "epoch: 342, loss: 2.2554352283477783\n",
      "epoch: 343, loss: 2.255431652069092\n",
      "epoch: 344, loss: 2.2554283142089844\n",
      "epoch: 345, loss: 2.255424976348877\n",
      "epoch: 346, loss: 2.2554214000701904\n",
      "epoch: 347, loss: 2.255418062210083\n",
      "epoch: 348, loss: 2.2554144859313965\n",
      "epoch: 349, loss: 2.255411148071289\n",
      "epoch: 350, loss: 2.2554075717926025\n",
      "epoch: 351, loss: 2.255404233932495\n",
      "epoch: 352, loss: 2.2554008960723877\n",
      "epoch: 353, loss: 2.2553975582122803\n",
      "epoch: 354, loss: 2.255394220352173\n",
      "epoch: 355, loss: 2.2553906440734863\n",
      "epoch: 356, loss: 2.255387544631958\n",
      "epoch: 357, loss: 2.2553839683532715\n",
      "epoch: 358, loss: 2.255380392074585\n",
      "epoch: 359, loss: 2.2553772926330566\n",
      "epoch: 360, loss: 2.25537371635437\n",
      "epoch: 361, loss: 2.2553703784942627\n",
      "epoch: 362, loss: 2.255366802215576\n",
      "epoch: 363, loss: 2.255363702774048\n",
      "epoch: 364, loss: 2.2553601264953613\n",
      "epoch: 365, loss: 2.255356788635254\n",
      "epoch: 366, loss: 2.2553536891937256\n",
      "epoch: 367, loss: 2.255350112915039\n",
      "epoch: 368, loss: 2.2553470134735107\n",
      "epoch: 369, loss: 2.255343437194824\n",
      "epoch: 370, loss: 2.255340576171875\n",
      "epoch: 371, loss: 2.2553372383117676\n",
      "epoch: 372, loss: 2.255333662033081\n",
      "epoch: 373, loss: 2.255330801010132\n",
      "epoch: 374, loss: 2.255326986312866\n",
      "epoch: 375, loss: 2.2553234100341797\n",
      "epoch: 376, loss: 2.2553203105926514\n",
      "epoch: 377, loss: 2.255317211151123\n",
      "epoch: 378, loss: 2.2553138732910156\n",
      "epoch: 379, loss: 2.2553107738494873\n",
      "epoch: 380, loss: 2.25530743598938\n",
      "epoch: 381, loss: 2.2553043365478516\n",
      "epoch: 382, loss: 2.2553012371063232\n",
      "epoch: 383, loss: 2.2552976608276367\n",
      "epoch: 384, loss: 2.2552945613861084\n",
      "epoch: 385, loss: 2.255290985107422\n",
      "epoch: 386, loss: 2.2552878856658936\n",
      "epoch: 387, loss: 2.2552850246429443\n",
      "epoch: 388, loss: 2.255281686782837\n",
      "epoch: 389, loss: 2.2552783489227295\n",
      "epoch: 390, loss: 2.255274772644043\n",
      "epoch: 391, loss: 2.2552716732025146\n",
      "epoch: 392, loss: 2.2552685737609863\n",
      "epoch: 393, loss: 2.255265712738037\n",
      "epoch: 394, loss: 2.2552623748779297\n",
      "epoch: 395, loss: 2.2552592754364014\n",
      "epoch: 396, loss: 2.255255699157715\n",
      "epoch: 397, loss: 2.2552525997161865\n",
      "epoch: 398, loss: 2.255249500274658\n",
      "epoch: 399, loss: 2.255246162414551\n",
      "epoch: 400, loss: 2.2552430629730225\n",
      "epoch: 401, loss: 2.2552402019500732\n",
      "epoch: 402, loss: 2.2552366256713867\n",
      "epoch: 403, loss: 2.2552335262298584\n",
      "epoch: 404, loss: 2.25523042678833\n",
      "epoch: 405, loss: 2.2552273273468018\n",
      "epoch: 406, loss: 2.2552242279052734\n",
      "epoch: 407, loss: 2.255221366882324\n",
      "epoch: 408, loss: 2.2552177906036377\n",
      "epoch: 409, loss: 2.2552149295806885\n",
      "epoch: 410, loss: 2.25521183013916\n",
      "epoch: 411, loss: 2.255208730697632\n",
      "epoch: 412, loss: 2.2552053928375244\n",
      "epoch: 413, loss: 2.255202531814575\n",
      "epoch: 414, loss: 2.2551991939544678\n",
      "epoch: 415, loss: 2.2551963329315186\n",
      "epoch: 416, loss: 2.2551934719085693\n",
      "epoch: 417, loss: 2.255190134048462\n",
      "epoch: 418, loss: 2.2551867961883545\n",
      "epoch: 419, loss: 2.2551839351654053\n",
      "epoch: 420, loss: 2.255181074142456\n",
      "epoch: 421, loss: 2.2551777362823486\n",
      "epoch: 422, loss: 2.2551746368408203\n",
      "epoch: 423, loss: 2.255171775817871\n",
      "epoch: 424, loss: 2.2551684379577637\n",
      "epoch: 425, loss: 2.2551653385162354\n",
      "epoch: 426, loss: 2.255162477493286\n",
      "epoch: 427, loss: 2.255159378051758\n",
      "epoch: 428, loss: 2.2551565170288086\n",
      "epoch: 429, loss: 2.2551534175872803\n",
      "epoch: 430, loss: 2.255150318145752\n",
      "epoch: 431, loss: 2.2551474571228027\n",
      "epoch: 432, loss: 2.2551441192626953\n",
      "epoch: 433, loss: 2.255141258239746\n",
      "epoch: 434, loss: 2.2551381587982178\n",
      "epoch: 435, loss: 2.2551352977752686\n",
      "epoch: 436, loss: 2.2551321983337402\n",
      "epoch: 437, loss: 2.255129098892212\n",
      "epoch: 438, loss: 2.2551262378692627\n",
      "epoch: 439, loss: 2.2551231384277344\n",
      "epoch: 440, loss: 2.2551205158233643\n",
      "epoch: 441, loss: 2.255117654800415\n",
      "epoch: 442, loss: 2.2551145553588867\n",
      "epoch: 443, loss: 2.2551114559173584\n",
      "epoch: 444, loss: 2.255108594894409\n",
      "epoch: 445, loss: 2.255105495452881\n",
      "epoch: 446, loss: 2.2551023960113525\n",
      "epoch: 447, loss: 2.2550995349884033\n",
      "epoch: 448, loss: 2.255096673965454\n",
      "epoch: 449, loss: 2.255093574523926\n",
      "epoch: 450, loss: 2.2550907135009766\n",
      "epoch: 451, loss: 2.2550878524780273\n",
      "epoch: 452, loss: 2.255084753036499\n",
      "epoch: 453, loss: 2.25508189201355\n",
      "epoch: 454, loss: 2.2550790309906006\n",
      "epoch: 455, loss: 2.2550761699676514\n",
      "epoch: 456, loss: 2.255073070526123\n",
      "epoch: 457, loss: 2.255070686340332\n",
      "epoch: 458, loss: 2.2550673484802246\n",
      "epoch: 459, loss: 2.2550647258758545\n",
      "epoch: 460, loss: 2.255061388015747\n",
      "epoch: 461, loss: 2.255058765411377\n",
      "epoch: 462, loss: 2.2550559043884277\n",
      "epoch: 463, loss: 2.2550528049468994\n",
      "epoch: 464, loss: 2.2550501823425293\n",
      "epoch: 465, loss: 2.25504732131958\n",
      "epoch: 466, loss: 2.2550442218780518\n",
      "epoch: 467, loss: 2.2550415992736816\n",
      "epoch: 468, loss: 2.2550384998321533\n",
      "epoch: 469, loss: 2.255035638809204\n",
      "epoch: 470, loss: 2.255033254623413\n",
      "epoch: 471, loss: 2.2550299167633057\n",
      "epoch: 472, loss: 2.2550272941589355\n",
      "epoch: 473, loss: 2.2550241947174072\n",
      "epoch: 474, loss: 2.255021572113037\n",
      "epoch: 475, loss: 2.255018949508667\n",
      "epoch: 476, loss: 2.2550158500671387\n",
      "epoch: 477, loss: 2.2550129890441895\n",
      "epoch: 478, loss: 2.2550101280212402\n",
      "epoch: 479, loss: 2.255007743835449\n",
      "epoch: 480, loss: 2.255004644393921\n",
      "epoch: 481, loss: 2.2550017833709717\n",
      "epoch: 482, loss: 2.2549989223480225\n",
      "epoch: 483, loss: 2.2549962997436523\n",
      "epoch: 484, loss: 2.254993438720703\n",
      "epoch: 485, loss: 2.254990577697754\n",
      "epoch: 486, loss: 2.2549877166748047\n",
      "epoch: 487, loss: 2.2549850940704346\n",
      "epoch: 488, loss: 2.2549824714660645\n",
      "epoch: 489, loss: 2.2549796104431152\n",
      "epoch: 490, loss: 2.254976749420166\n",
      "epoch: 491, loss: 2.254973888397217\n",
      "epoch: 492, loss: 2.2549712657928467\n",
      "epoch: 493, loss: 2.2549684047698975\n",
      "epoch: 494, loss: 2.2549655437469482\n",
      "epoch: 495, loss: 2.254962921142578\n",
      "epoch: 496, loss: 2.254960060119629\n",
      "epoch: 497, loss: 2.254957437515259\n",
      "epoch: 498, loss: 2.2549548149108887\n",
      "epoch: 499, loss: 2.2549519538879395\n",
      "tensor(2.2550, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(500):\n",
    "    # forward pass\n",
    "\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    logits = x_enc @ weights\n",
    "    \n",
    "    # This is cross entropy loss\n",
    "    # count = logits.exp()\n",
    "    # probs = count / count.sum(dim=1,keepdim=True)\n",
    "    # loss = -probs[torch.arange(total_trigrams),ys].log().mean()\n",
    "\n",
    "    loss = loss_fn(logits,ys) + 0.01 * (weights**2).mean()\n",
    "\n",
    "    print(f'epoch: {epoch}, loss: {loss.item()}')\n",
    "\n",
    "    # backward pass\n",
    "    weights.grad = None\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # update weights\n",
    "    weights.data -= 10 * weights.grad\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word names with k \n",
      " -----------------\n",
      "kali\n",
      "kah\n",
      "kaylellya\n",
      "komoh\n",
      "kaysayah\n",
      "kagmigale\n",
      "kharte\n",
      "kocsnes\n",
      "kalaita\n",
      "kaien\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inferencing\n",
    "\n",
    "rand_gen = torch.Generator().manual_seed(12141234)\n",
    "\n",
    "def make_word(start_with : str) -> str:\n",
    "    start = [0,char2idx[start_with]]\n",
    "    out = [start_with]\n",
    "\n",
    "    while True:\n",
    "        x_enc = F.one_hot(torch.tensor(start), num_classes=27).float().flatten()\n",
    "        logits = x_enc @ weights\n",
    "        count = logits.exp()\n",
    "        probs = count / count.sum()\n",
    "\n",
    "        next_char = torch.multinomial(probs,1, generator=rand_gen, replacement=True).item()\n",
    "\n",
    "        if next_char == 0:\n",
    "            break\n",
    "\n",
    "        out.append(idx2char[next_char])\n",
    "        start = [start[1],next_char]\n",
    "    \n",
    "    print(''.join(out))\n",
    "\n",
    "\n",
    "\n",
    "_char = 'k'\n",
    "print(f'Word names with {_char} \\n -----------------')\n",
    "for i in range(10):\n",
    "    make_word(_char)\n",
    "print()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7afcb4a04d08665c2529ae0dc1d5ec01f94baaa4a57a18e0d9f75f04722700b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
