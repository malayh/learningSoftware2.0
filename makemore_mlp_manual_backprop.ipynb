{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers.makemore_helpers import make_char2idx_map, build_dataset\n",
    "\n",
    "words = open(r'data\\names.txt','r').read().splitlines()\n",
    "char2idx = make_char2idx_map(words)\n",
    "idx2char = {v: k for k, v in char2idx.items()}\n",
    "\n",
    "block_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: 182778 22633 22735\n",
      "Shapes: torch.Size([182778, 3]) torch.Size([182778])\n"
     ]
    }
   ],
   "source": [
    "train_count = int(len(words) * 0.8)\n",
    "n_1 = int(len(words) * 0.9)\n",
    "\n",
    "x_train, y_train = build_dataset(words[:train_count], block_size, char2idx)\n",
    "x_val, y_val = build_dataset(words[train_count:n_1], block_size, char2idx)\n",
    "x_test, y_test = build_dataset(words[n_1:], block_size, char2idx)\n",
    "\n",
    "total_train = len(x_train)\n",
    "total_val = len(x_val)\n",
    "total_test = len(x_test)\n",
    "\n",
    "print(f\"Sizes:\", total_train, total_val, total_test)\n",
    "print(\"Shapes:\", x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "rand_gen = torch.Generator().manual_seed(42)\n",
    "embedding_size = 10\n",
    "n_hidden = 200\n",
    "vocab_size = len(char2idx)\n",
    "\n",
    "embedding_matrix = torch.randn((vocab_size, 10), generator=rand_gen)\n",
    "# Because we tanh to remain in the active region\n",
    "W1 = torch.randn((embedding_size * block_size, n_hidden), generator=rand_gen) *  ( (5/3) / (embedding_size * block_size)**0.5) # this is kaiming initialization\n",
    "b1 = torch.randn(n_hidden, generator=rand_gen) * 0.001 # Because we tanh to remain in the active region\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=rand_gen) * 0.01 # Because we want smaller logits\n",
    "b2 = torch.randn(vocab_size, generator=rand_gen) * 0.1 # Because we want smaller logits\n",
    "\n",
    "batch_norm_gain = torch.randn((1,n_hidden)) * 0.1 + 1\n",
    "batch_norm_bias = torch.randn((1,n_hidden)) * 0.1\n",
    "\n",
    "mean_running = torch.zeros((1,n_hidden))\n",
    "std_running = torch.ones((1,n_hidden))\n",
    "\n",
    "params = [embedding_matrix, W1, W2, b2, b1, batch_norm_gain, batch_norm_bias]\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, x_train.shape[0], (batch_size,), generator=rand_gen)\n",
    "xs, ys = x_train[ix], y_train[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gradients(name, manual_gradient, target_tensor):\n",
    "  exact_matches = torch.all(manual_gradient == target_tensor.grad).item()\n",
    "  appoximate_clossness = torch.allclose(manual_gradient, target_tensor.grad)\n",
    "  max_diff = (manual_gradient - target_tensor.grad).abs().max().item()\n",
    "  \n",
    "  print(f'{name:15s} | exact: {str(exact_matches):5s} | approximate: {str(appoximate_clossness):5s} | maxdiff: {max_diff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Forward pass\n",
    "# ----------------------------------------\n",
    "\n",
    "embedding = embedding_matrix[xs] # embed the characters into vectors\n",
    "emb_concat = embedding.view(embedding.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "# Linear layer 1\n",
    "pre_act_1_before_batch_norm = emb_concat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "# BatchNorm layer\n",
    "bn_mean_i_th = (1/batch_size) * pre_act_1_before_batch_norm.sum(0, keepdim=True)\n",
    "bn_diff = pre_act_1_before_batch_norm - bn_mean_i_th                #\n",
    "bn_diff_sqr = bn_diff**2                                            #\n",
    "# note: Bessel's correction (dividing by n-1, not n)                # Calculate the variance\n",
    "bn_variance = 1/(batch_size-1)*(bn_diff_sqr).sum(0, keepdim=True)   #\n",
    "bn_variance_inv = (bn_variance + 1e-5)**-0.5                        #\n",
    "bn_raw_out = bn_diff * bn_variance_inv\n",
    "\n",
    "act_batch_norm = batch_norm_gain * bn_raw_out + batch_norm_bias\n",
    "\n",
    "\n",
    "# Non-linearity\n",
    "activation_1 = torch.tanh(act_batch_norm) # hidden layer\n",
    "\n",
    "# Linear layer 2\n",
    "logits = activation_1 @ W2 + b2 # output layer\n",
    "\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability. Because since we are using exp, we want to the values to be exploding to infinity\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv # Doing count/count_sum\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(batch_size), ys].mean()\n",
    "\n",
    "\n",
    "# pytorch backward pass\n",
    "intermediate_variables = [embedding, emb_concat, pre_act_1_before_batch_norm, bn_mean_i_th, bn_diff, bn_diff_sqr, bn_variance, bn_variance_inv, bn_raw_out, act_batch_norm, activation_1, logits, logit_maxes, norm_logits, counts, counts_sum, counts_sum_inv, probs, logprobs, loss]\n",
    "for v in intermediate_variables:\n",
    "    v.retain_grad()\n",
    "\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "\n",
    "loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "activation_1    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "act_batch_norm  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_gain         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_bias         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_raw_out      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_variance_inv | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_variance     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_diff_sqr     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_diff         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_mean_i_th    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "pre_act_1_before_batch_norm | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb_concat      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embedding       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embedding_matrix | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate gradients manually\n",
    "\n",
    "# logprobs is a matrix of size (batch_size, vocab_size). For us: (32,27)\n",
    "# logprobs[range(batch_size), ys] is a vector of size (batch_size,). For us: (32,)\n",
    "# For every row in logprobs, ^ this line plucks out log probs of correct characters(indexed by ys)\n",
    "\n",
    "# Loss = -logprobs[range(batch_size), ys].mean() \n",
    "# loss = -(a+b+c.....)/batch_size ( where a,b,c are the logprobs of correct characters)\n",
    "# dloss/dlogprobs = -1/batch_size for plucked out logprobs, and 0 for others\n",
    "\n",
    "d_logprobs = torch.zeros_like(logprobs) # (32,27)\n",
    "d_logprobs[range(batch_size), ys] = -1/batch_size\n",
    "compare_gradients('logprobs', d_logprobs, logprobs)\n",
    "\n",
    "\n",
    "# logprobs = log(probs)\n",
    "# d_logprobs/d_probs = 1/probs (dlog/dx = 1/x)\n",
    "# d_loss/d_probs = d_loss/d_logprobs * d_logprobs/d_probs = d_logprobs * 1/probs (chain rule)\n",
    "d_probs = d_logprobs * (1/probs)\n",
    "compare_gradients('probs', d_probs, probs)\n",
    "\n",
    "\n",
    "# counts.shape, counts_sum_inv.shape = (32,27), (32,1)\n",
    "# c = a * b\n",
    "# a[3x3] * b[3x1] -> broadcast -> c[3x3] as follows:\n",
    "# a11 * b1, a12 * b1, a13 * b1\n",
    "# a21 * b1, a22 * b1, a23 * b1\n",
    "# a31 * b1, a32 * b1, a33 * b1\n",
    "#\n",
    "# probs = counts * counts_sum_inv\n",
    "\n",
    "# d_probs/d_count_sum_ind = counts (d x*y / dy = x)\n",
    "# why sum? because we want to sum up the gradients each copy caused due to broadcasting\n",
    "d_counts_sum_inv = (counts * d_probs).sum(1, keepdims=True)\n",
    "compare_gradients('counts_sum_inv', d_counts_sum_inv, counts_sum_inv)\n",
    "\n",
    "# Part 1 of d_count: counts countributes to probs and counts_sum. Two branches\n",
    "d_counts = counts_sum_inv * d_probs # (? why not summing here)\n",
    "\n",
    "\n",
    "# d_count_sum/d_count_sum_inv = -counts_sum**-2 (d x^-1 / dx = -1/x**2)\n",
    "d_count_sum = (-counts_sum**-2) * d_counts_sum_inv\n",
    "compare_gradients('counts_sum', d_count_sum, counts_sum)\n",
    "\n",
    "# Differentiate the broadcaste: counts_sum = counts.sum(1, keepdims=True)\n",
    "# a11 + a12 + a13 = b1   |      [da11, da12 , da13] = [db1, db1, db1]      | \n",
    "# a21 + a22 + a23 = b2   | =>   [da21, da22 , da23] = [db2, db2, db2]      | Because d(x+y+z)/dx = 1, because of chain rule you multiply the gradient with 1  \n",
    "# a31 + a32 + a33 = b3   |      [da31, da32 , da33] = [db3, db3, db3]      |\n",
    "\n",
    "# Part 2 of d_count:\n",
    "d_counts += torch.ones_like(counts) * d_count_sum # (32,27)\n",
    "compare_gradients('counts', d_counts, counts)\n",
    "\n",
    "\n",
    "# d_count/d_norm_logit = exp(norm_logit) (d e**x / dx = e**x)\n",
    "d_norm_logits = norm_logits.exp() * d_counts\n",
    "compare_gradients('norm_logits', d_norm_logits, norm_logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# norm_logits = logits - logit_maxes\n",
    "# norm_logits.shape, logits.shape, logit_maxes.shape = (32,27), (32,27), (32,1) => There is a broadcast\n",
    "# c11, c12, c13     a11, a12, a13    b1\n",
    "# c21, c22, c23  =  a21, a22, a23 -  b2\n",
    "# c31, c32, c33     a31, a32, a33    b3\n",
    "\n",
    "# c11 = a11 - b1\n",
    "# d_c11 (local derivative) = (d_c11 / d_a11) + (d_c11 / d_b1) = -b1 + 1\n",
    "# And we have to do a sum across the columns, because we are summing up the gradients of each copy due to broadcasting\n",
    "\n",
    "# Branch 1 for d_logits\n",
    "d_logits = d_norm_logits.clone()\n",
    "d_logit_maxes = (-d_norm_logits).sum(1, keepdims=True)\n",
    "compare_gradients('logit_maxes', d_logit_maxes, logit_maxes)\n",
    "\n",
    "\n",
    "# d_logits += torch.ones_like(logits) * d_logit_maxes # This is equivalent to the line below\n",
    "d_logits += F.one_hot( logits.max(1).indices, num_classes=logits.shape[1]).float() * d_logit_maxes\n",
    "compare_gradients('logits', d_logits, logits)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------- I DON'T UNDERSTAND THE FORMULAS GRADIENT IN THIS PART ----------------------------------\n",
    "# logits = activation_1 @ W2 + b2\n",
    "# activation_1.shape, W2.shape,d_logits.shape, b2.shape = (32, 200), (200, 27), (32,27) (27,)\n",
    "\n",
    "# TRICK to get the formula for d_activation_1/d_logits\n",
    "# - Shape of d_activation has to be same as activation_1\n",
    "# - d_activation is some kind of matrix multiplication between d_logits and W2 such the shapes work out\n",
    "# - d_W2 is some kind of matrix multiplication between activation_1 and d_logits such the shapes work out\n",
    "\n",
    "\n",
    "d_activation_1 = d_logits @ W2.T\n",
    "compare_gradients('activation_1', d_activation_1, activation_1)\n",
    "\n",
    "d_W2 = activation_1.T @ d_logits\n",
    "compare_gradients('W2', d_W2, W2)\n",
    "\n",
    "d_b2 = d_logits.sum(0, keepdims=True)\n",
    "compare_gradients('b2', d_b2, b2)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# d_act_batch_norm/d_activation_1. We already know d_activation_1/d_logits, which we need to pass throgh the tanh to get d_act_batch_norm\n",
    "d_act_batch_norm = d_activation_1 * (1 - torch.tanh(act_batch_norm)**2)\n",
    "compare_gradients('act_batch_norm', d_act_batch_norm, act_batch_norm)\n",
    "\n",
    "\n",
    "\n",
    "# for the batch norm\n",
    "d_bn_gain = (bn_raw_out * d_act_batch_norm).sum(0, keepdims=True)\n",
    "compare_gradients('bn_gain', d_bn_gain, batch_norm_gain)\n",
    "\n",
    "d_bn_bias = d_act_batch_norm.sum(0, keepdims=True)\n",
    "compare_gradients('bn_bias', d_bn_bias, batch_norm_bias)\n",
    "\n",
    "d_bn_raw_out = batch_norm_gain * d_act_batch_norm\n",
    "compare_gradients('bn_raw_out', d_bn_raw_out, bn_raw_out)\n",
    "\n",
    "\n",
    "\n",
    "# bn_raw_out = bn_diff * bn_variance_inv\n",
    "# Part 1 of d_bn_diff\n",
    "d_bn_diff = bn_variance_inv * d_bn_raw_out\n",
    "\n",
    "d_bn_variance_inv = (bn_diff * d_bn_raw_out).sum(0, keepdims=True)\n",
    "compare_gradients('bn_variance_inv', d_bn_variance_inv, bn_variance_inv)    \n",
    "\n",
    "# bn_variance_inv = (bn_variance + 1e-5)**-0.5 \n",
    "d_bn_variance = -0.5 * (bn_variance + 1e-5)**-1.5 * d_bn_variance_inv\n",
    "compare_gradients('bn_variance', d_bn_variance, bn_variance)\n",
    "\n",
    "\n",
    "# bn_variance = 1/(batch_size-1)*(bn_diff_sqr).sum(0, keepdim=True) \n",
    "#        | a11    a12 |                          | b1 |           | (a11+a21)/n |\n",
    "# 1/n *  | a21    a22 |.sum(0,keepdim=True) ==>  | b2 | * 1/n ==> | (a12+a22)/n | \n",
    "#  \n",
    "d_bn_diff_sqr = (1.0/(batch_size-1))*torch.ones_like(bn_diff_sqr) * d_bn_variance\n",
    "compare_gradients('bn_diff_sqr', d_bn_diff_sqr, bn_diff_sqr)\n",
    "\n",
    "# bn_diff_sqr = bn_diff**2\n",
    "# Part 2 of d_bn_diff\n",
    "d_bn_diff += 2*bn_diff * d_bn_diff_sqr\n",
    "compare_gradients('bn_diff', d_bn_diff, bn_diff)\n",
    "\n",
    "# bn_diff = pre_act_1_before_batch_norm - bn_mean_i_th  (Broadcasting in forward pass <=> Sum of gradients in backward pass)\n",
    "d_pre_act_1_before_batch_norm =  d_bn_diff.clone()\n",
    "d_bn_mean_i_th = (-torch.ones_like(bn_diff) * d_bn_diff).sum(0, keepdims=True)\n",
    "compare_gradients('bn_mean_i_th', d_bn_mean_i_th, bn_mean_i_th)\n",
    "\n",
    "# bn_mean_i_th = (1/batch_size) * pre_act_1_before_batch_norm.sum(0, keepdim=True)\n",
    "d_pre_act_1_before_batch_norm += (1/batch_size) * torch.ones_like(pre_act_1_before_batch_norm) * d_bn_mean_i_th\n",
    "compare_gradients('pre_act_1_before_batch_norm', d_pre_act_1_before_batch_norm, pre_act_1_before_batch_norm)\n",
    "\n",
    "\n",
    "# pre_act_1_before_batch_norm = emb_concat @ W1 + b1 # hidden layer pre-activation\n",
    "d_W1 = emb_concat.T @ d_pre_act_1_before_batch_norm\n",
    "compare_gradients('W1', d_W1, W1)\n",
    "\n",
    "d_emb_concat = d_pre_act_1_before_batch_norm @ W1.T\n",
    "compare_gradients('emb_concat', d_emb_concat, emb_concat)\n",
    "\n",
    "d_b1 = d_pre_act_1_before_batch_norm.sum(0)\n",
    "compare_gradients('b1', d_b1, b1)\n",
    "\n",
    "\n",
    "# emb_concat = embedding.view(embedding.shape[0], -1)\n",
    "d_embedding = d_emb_concat.view(embedding.shape)\n",
    "compare_gradients('embedding', d_embedding, embedding)\n",
    "\n",
    "\n",
    "# embedding = embedding_matrix[xs] \n",
    "d_embedding_matrix = torch.zeros_like(embedding_matrix)\n",
    "for i in range(xs.shape[0]):\n",
    "    for j in range(xs.shape[1]):\n",
    "        d_embedding_matrix[xs[i,j]] += d_embedding[i,j] \n",
    "\n",
    "compare_gradients('embedding_matrix', d_embedding_matrix, embedding_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7afcb4a04d08665c2529ae0dc1d5ec01f94baaa4a57a18e0d9f75f04722700b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
